{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zZozDxZ8_sjz",
    "outputId": "6b67e182-2fdb-4f7e-a4a2-c9a103244032"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/krushilpatel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/krushilpatel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/krushilpatel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import log10\n",
    "import re\n",
    "from emoji import UNICODE_EMOJI\n",
    "import emoji\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQJ8vOztIxWL",
    "outputId": "6a3c47a5-0f90-433f-f409-b282cba2c67c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet label\n",
      "0  The CDC currently reports 99031 deaths. In gen...  real\n",
      "1  States reported 1121 deaths a small rise from ...  real\n",
      "2  Politically Correct Woman (Almost) Uses Pandem...  fake\n",
      "3  #IndiaFightsCorona: We have 1524 #COVID testin...  real\n",
      "4  Populous states can generate large case counts...  real\n",
      "5  Covid Act Now found \"on average each person in...  real\n",
      "6  If you tested positive for #COVID19 and have n...  real\n",
      "7  Obama Calls Trump‚Äôs Coronavirus Response A Cha...  fake\n",
      "8  ???Clearly, the Obama administration did not l...  fake\n",
      "9  Retraction‚ÄîHydroxychloroquine or chloroquine w...  fake\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"data.xlsx\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqMbjLS2JOh-"
   },
   "source": [
    "#Preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Kh491DjeJNw1"
   },
   "outputs": [],
   "source": [
    "def super_simple_preprocess(text):\n",
    "  # lowercase\n",
    "  text = text.lower()\n",
    "  # remove non alphanumeric characters\n",
    "  text = re.sub('[^A-Za-z0-9 ]+',' ', text)\n",
    "  return text\n",
    "def process_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word)\n",
    "                         for word in tokens\n",
    "                         if word.lower() not in stop_words]\n",
    "\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9zwZKYLA_BH"
   },
   "source": [
    "## Handling #Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "B2cn24cUyDBX"
   },
   "outputs": [],
   "source": [
    "def memo(f):\n",
    "    \"Memoize function f.\"\n",
    "    table = {}\n",
    "    def fmemo(*args):\n",
    "        if args not in table:\n",
    "            table[args] = f(*args)\n",
    "        return table[args]\n",
    "    fmemo.memo = table\n",
    "    return fmemo\n",
    "\n",
    "def test(verbose=None):\n",
    "    \"\"\"Run some tests, taken from the chapter.\n",
    "    Since the hillclimbing algorithm is randomized, some tests may fail.\"\"\"\n",
    "    import doctest\n",
    "    doctest.testfile('ngrams-test.txt', verbose=verbose)\n",
    "@memo\n",
    "def segment(text):\n",
    "    \"Return a list of words that is the best segmentation of text.\"\n",
    "    text = text.lower()\n",
    "    if not text:\n",
    "      return []\n",
    "    candidates = ([first]+segment(rem) for first,rem in splits(text))\n",
    "    return max(candidates, key=Pwords)\n",
    "\n",
    "def splits(text, L=20):\n",
    "    \"Return a list of all possible (first, rem) pairs, len(first)<=L.\"\n",
    "    return [(text[:i+1], text[i+1:])\n",
    "            for i in range(min(len(text), L))]\n",
    "\n",
    "def final_output(text):\n",
    "    seg = segment(str(text)[1:])\n",
    "    if(len(seg)>0):\n",
    "      return \" \".join(seg)\n",
    "    return \" \"\n",
    "\n",
    "def Pwords(words):\n",
    "    \"The Naive Bayes probability of a sequence of words.\"\n",
    "    return product(Pw(w) for w in words)\n",
    "\n",
    "#### Support functions (p. 224)\n",
    "\n",
    "def product(nums):\n",
    "    \"Return the product of a sequence of numbers.\"\n",
    "    ans=0\n",
    "    for i in nums:\n",
    "      ans+=log10(i)\n",
    "    return ans\n",
    "\n",
    "class Pdist(dict):\n",
    "    \"A probability distribution estimated from counts in datafile.\"\n",
    "    def __init__(self, data=[], N=None, missingfn=None):\n",
    "        for d in data:\n",
    "            if(len(d)!=2):\n",
    "              continue\n",
    "            self[d[0]] = self.get(d[0], 0) + int(d[1])\n",
    "        self.N = float(N or sum(self.itervalues()))\n",
    "        self.missingfn = missingfn or (lambda k, N: 1./N)\n",
    "    def __call__(self, key):\n",
    "        if key in self: return self[key]/self.N\n",
    "        else: return self.missingfn(key, self.N)\n",
    "\n",
    "def datafile(name, sep='\\t'):\n",
    "    \"Read key,value pairs from file.\"\n",
    "    final = []\n",
    "    f = open(name,'r')\n",
    "    while True:\n",
    "      line = f.readline()\n",
    "      if not line:\n",
    "        break\n",
    "      final.append(line.split('\\t'))\n",
    "    return final\n",
    "\n",
    "def avoid_long_words(key, N):\n",
    "    \"Estimate the probability of an unknown word.\"\n",
    "    return 10./(N * 10**len(key))\n",
    "\n",
    "N = 1024908267229 ## Number of tokens\n",
    "Pw  = Pdist(datafile('count_1w.txt'), N, avoid_long_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FMuvVaBJZXe"
   },
   "source": [
    "## Handling URL & emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptAAgyU3A9zP",
    "outputId": "c80a32e7-fecd-4963-c496-4b511bb914d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am  Let's grab some lunch: :pizza: or :steaming_bowl: or :bento_box:?\n"
     ]
    }
   ],
   "source": [
    "def remove_urls(text, replacement_text=\"\"):\n",
    "    # Define a regex pattern to match URLs\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    # Use the sub() method to replace URLs with the specified replacement text\n",
    "    text_without_urls = url_pattern.sub(replacement_text, text)\n",
    "    text_without_urls_emoji = emoji.demojize(text_without_urls)\n",
    "    return text_without_urls_emoji\n",
    "print(remove_urls(\"i am www.useless.com Let's grab some lunch: üçï or üçú or üç±?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "zMIGWYeyNJfS"
   },
   "outputs": [],
   "source": [
    "def my_preprocessor(text):\n",
    "  text = remove_urls(text)\n",
    "  text = re.sub(r'#[A-Za-z0-9]+', lambda m: final_output(m.group()), text)\n",
    "  text = super_simple_preprocess(text)\n",
    "  # text = process_text(text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "lBkM0VQMNwvR",
    "outputId": "2c8ad724-73e8-46b4-9910-4c4c055de83b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet label  \\\n",
      "5  Covid Act Now found \"on average each person in...  real   \n",
      "6  If you tested positive for #COVID19 and have n...  real   \n",
      "7  Obama Calls Trump‚Äôs Coronavirus Response A Cha...  fake   \n",
      "8  ???Clearly, the Obama administration did not l...  fake   \n",
      "9  Retraction‚ÄîHydroxychloroquine or chloroquine w...  fake   \n",
      "\n",
      "                                  preprocessed_tweet  \n",
      "5  covid act now found  on average each person in...  \n",
      "6  if you tested positive for covid19 and have no...  \n",
      "7  obama calls trump s coronavirus response a cha...  \n",
      "8   clearly  the obama administration did not lea...  \n",
      "9  retraction hydroxychloroquine or chloroquine w...  \n"
     ]
    }
   ],
   "source": [
    "df['preprocessed_tweet'] = df['tweet'].apply(my_preprocessor)\n",
    "print(df[5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gq5sRNxSImTp",
    "outputId": "c5a9eba9-83df-4210-cda8-bbeef2c09f31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#IndiaFightsCorona: We have 1524 #COVID testing laboratories in India and as on 25th August 2020 36827520 tests have been done : @ProfBhargava DG @ICMRDELHI #StaySafe #IndiaWillWin https://t.co/Yh3ZxknnhZ\n",
      "india fights corona  we have 1524 co vid testing laboratories in india and as on 25th august 2020 36827520 tests have been done    profbhargava dg  icmrdelhi stay safe india will win \n",
      "india fights corona\n"
     ]
    }
   ],
   "source": [
    "print(df[\"tweet\"][3])\n",
    "print(df[\"preprocessed_tweet\"][3])\n",
    "print(final_output(\"#IndiaFightsCorona\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "0hhalcLUJpDO"
   },
   "outputs": [],
   "source": [
    "X = df['preprocessed_tweet']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=49)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Xk7GQYITR4NS"
   },
   "outputs": [],
   "source": [
    "# Create DataFrames for train, test, and validation sets\n",
    "train_df = pd.DataFrame({'X': X_train, 'y': y_train})\n",
    "test_df = pd.DataFrame({'X': X_test, 'y': y_test})\n",
    "val_df = pd.DataFrame({'X': X_val, 'y': y_val})\n",
    "# Export DataFrames to CSV files\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "test_df.to_csv('test_data.csv', index=False)\n",
    "val_df.to_csv('validate_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "GHfn6zJGrxpn"
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "# Fit and transform the 'text' column to get the tokens\n",
    "tokens = count_vectorizer.fit_transform(pd.concat([train_df, val_df], ignore_index=True)['X'])\n",
    "# Get the feature names (vocabulary)\n",
    "vocabulary = count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "RJSg1cbfPEuc"
   },
   "outputs": [],
   "source": [
    "#Train\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(vocabulary=vocabulary)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the DataFrame with TF-IDF vector representations\n",
    "dbfile = open('Train', 'ab')\n",
    "pickle.dump(tfidf_df, dbfile)\n",
    "dbfile.close()\n",
    "#Validate\n",
    "# Create TF-IDF vectorizer with custom vocabulary\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(X_val)\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the DataFrame with TF-IDF vector representations\n",
    "dbfile = open('Validate', 'ab')\n",
    "pickle.dump(tfidf_df, dbfile)\n",
    "dbfile.close()\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(X_test)\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the DataFrame with TF-IDF vector representations\n",
    "dbfile = open('Test', 'ab')\n",
    "pickle.dump(tfidf_df, dbfile)\n",
    "dbfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
